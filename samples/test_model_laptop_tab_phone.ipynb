{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Testing Custom Mask R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import skimage.draw\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "from mrcnn.visualize import display_instances\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n",
    "# Import COCO config\n",
    "sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))  # To find local version\n",
    "import coco\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join('', \"mask_rcnn_coco.h5\")\n",
    "\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"dataset\\train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConfig(Config):\n",
    "    \"\"\"Configuration for training on the custom  dataset.\n",
    "    Derives from the base Config class and overrides some values.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"object\"\n",
    "\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    NUM_CLASSES = 1 + 4  # Background + persona, raqueta, hidrante and patineta\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 10\n",
    "\n",
    "    # Skip detections with < 90% confidence\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                93\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           coco\n",
      "NUM_CLASSES                    81\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Customdataset class. Same code is present in custom.py file also\n",
    "class CustomDataset(utils.Dataset):\n",
    "\n",
    "    def load_custom(self, dataset_dir, subset):\n",
    "\n",
    "        self.add_class(\"object\", 1, \"persona\")\n",
    "        self.add_class(\"object\", 2, \"hidrante\")\n",
    "        self.add_class(\"object\", 3, \"patineta\")\n",
    "        self.add_class(\"object\", 4, \"raqueta\")\n",
    "\n",
    "        assert subset in [\"train\", \"val\"]\n",
    "        dataset_dir = os.path.join(dataset_dir, subset)\n",
    "        annotations1 = json.load(open('D:/Mask_RCNN/Dataset/train/demo_json.json'))\n",
    "\n",
    "        annotations = list(annotations1.values())  # don't need the dict keys\n",
    "\n",
    "        annotations = [a for a in annotations if a['regions']]\n",
    "        \n",
    "        # Add images\n",
    "        for a in annotations:\n",
    "           \n",
    "            polygons = [r['shape_attributes'] for r in a['regions']] \n",
    "            objects = [s['region_attributes']['nombre'] for s in a['regions']]\n",
    "            print(\"Objetos:\",objects)\n",
    "            name_dict = {\"persona\": 1,\"hidrante\": 2,\"patineta\": 3 , \"raqueta\": 4}\n",
    "            num_ids = [name_dict[a] for a in objects]\n",
    "\n",
    "            print(\"numids\",num_ids)\n",
    "            image_path = os.path.join(dataset_dir, a['filename'])\n",
    "            image = skimage.io.imread(image_path)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            self.add_image(\n",
    "                \"object\", \n",
    "                image_id=a['filename'],  # use file name as a unique image id\n",
    "                path=image_path,\n",
    "                width=width, height=height,\n",
    "                polygons=polygons,\n",
    "                num_ids=num_ids\n",
    "                )\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "       \n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"object\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] != \"object\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "        num_ids = info['num_ids']\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
    "                        dtype=np.uint8)\n",
    "        for i, p in enumerate(info[\"polygons\"]):\n",
    "\n",
    "        \trr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
    "\n",
    "        \tmask[rr, cc, i] = 1\n",
    "\n",
    "        num_ids = np.array(num_ids, dtype=np.int32)\n",
    "        return mask, num_ids #np.ones([mask.shape[-1]], dtype=np.int32)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"object\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objetos: ['persona', 'hidrante', 'persona', 'persona']\n",
      "numids [1, 2, 1, 1]\n",
      "Objetos: ['persona', 'patineta']\n",
      "numids [1, 3]\n",
      "Objetos: ['raqueta', 'persona']\n",
      "numids [4, 1]\n",
      "Images: 3\n",
      "Classes: ['BG', 'persona', 'hidrante', 'patineta', 'raqueta']\n"
     ]
    }
   ],
   "source": [
    "# Inspect the model in training or inference modes values: 'inference' or 'training'\n",
    "TEST_MODE = \"inference\"\n",
    "ROOT_DIR = \"D:\\Mask_RCNN\\dataset\"\n",
    "\n",
    "def get_ax(rows=1, cols=1, size=16):\n",
    "  \"\"\"Return a Matplotlib Axes array to be used in all visualizations in the notebook.  Provide a central point to control graph sizes. Adjust the size attribute to control how big to render images\"\"\"\n",
    "  _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "  return ax\n",
    "\n",
    "# Load validation dataset\n",
    "# Must call before using the dataset\n",
    "CUSTOM_DIR = \"D:/Mask_RCNN/dataset\"\n",
    "dataset = CustomDataset()\n",
    "dataset.load_custom(CUSTOM_DIR, \"val\")\n",
    "dataset.prepare()\n",
    "print(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    D:\\Mask_RCNN\\mrcnn\\model.py:390 call  *\n        roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area)))\n    D:\\Mask_RCNN\\mrcnn\\model.py:341 log2_graph  *\n        return tf.log(x) / tf.log(2.0)\n\n    AttributeError: module 'tensorflow' has no attribute 'log'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-4911e3e0b769>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#config = CustomConfig()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Create model object in inference mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodellib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskRCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"inference\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mask_rcnn_coco.hy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Load weights trained on MS-COCO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Mask_RCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mode, config, model_dir)\u001b[0m\n\u001b[0;32m   1835\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1836\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_log_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1837\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1839\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Mask_RCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, mode, config)\u001b[0m\n\u001b[0;32m   2033\u001b[0m             \u001b[1;31m# Proposal classifier and BBox regressor heads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2034\u001b[0m             \u001b[0mmrcnn_class_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmrcnn_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmrcnn_bbox\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2035\u001b[1;33m                 fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, input_image_meta,\n\u001b[0m\u001b[0;32m   2036\u001b[0m                                      \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOOL_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2037\u001b[0m                                      \u001b[0mtrain_bn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN_BN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Mask_RCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mfpn_classifier_graph\u001b[1;34m(rois, feature_maps, image_meta, pool_size, num_classes, train_bn, fc_layers_size)\u001b[0m\n\u001b[0;32m    922\u001b[0m     \u001b[1;31m# ROI Pooling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m     \u001b[1;31m# Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m     x = PyramidROIAlign([pool_size, pool_size],\n\u001b[0m\u001b[0;32m    925\u001b[0m                         name=\"roi_align_classifier\")([rois, image_meta] + feature_maps)\n\u001b[0;32m    926\u001b[0m     \u001b[1;31m# Two 1024 FC layers (implemented with Conv2D for consistency)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    949\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    952\u001b[0m                                                 input_list)\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1088\u001b[0m           layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[0;32m   1089\u001b[0m         \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[0;32m   1091\u001b[0m             inputs, input_masks, args, kwargs)\n\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    861\u001b[0m           \u001b[1;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    668\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    D:\\Mask_RCNN\\mrcnn\\model.py:390 call  *\n        roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area)))\n    D:\\Mask_RCNN\\mrcnn\\model.py:341 log2_graph  *\n        return tf.log(x) / tf.log(2.0)\n\n    AttributeError: module 'tensorflow' has no attribute 'log'\n"
     ]
    }
   ],
   "source": [
    "#config = CustomConfig()\n",
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir='mask_rcnn_coco.hy', config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights('mask_rcnn_coco.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO weights Or, load the last model you trained\n",
    "weights_path = WEIGHTS_PATH\n",
    "# Load weights\n",
    "print(\"Loading weights \", weights_path)\n",
    "model.load_weights(weights_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we are ready for testing our model on any image.\n",
    "\n",
    "#RUN DETECTION\n",
    "image_id = random.choice(dataset.image_ids)\n",
    "#image_id = 'D:/MaskRCNN-aar/Dataset/val/1.jfif'\n",
    "print(\"image id is :\",image_id)\n",
    "image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n",
    "info = dataset.image_info[image_id]\n",
    "print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id,dataset.image_reference(image_id)))\n",
    "\n",
    "# Run object detection\n",
    "results = model.detect([image], verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "x = get_ax(1)\n",
    "r = results[0]\n",
    "ax = plt.gca()\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], dataset.class_names, r['scores'], ax=ax, title=\"Predictions\")\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "# This is for predicting images which are not present in dataset\n",
    "path_to_new_image = 'D:/MaskRCNN-aar/prueba01.jpg'\n",
    "image1 = mpimg.imread(path_to_new_image)\n",
    "\n",
    "# Run object detection\n",
    "print(len([image1]))\n",
    "results1 = model.detect([image1], verbose=1)\n",
    "\n",
    "# Display results\n",
    "ax = get_ax(1)\n",
    "r1 = results1[0]\n",
    "visualize.display_instances(image1, r1['rois'], r1['masks'], r1['class_ids'],\n",
    "dataset.class_names, r1['scores'], ax=ax, title=\"Predictions1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_new_image = 'D:/MaskRCNN-aar/prueba01.jpg'\n",
    "image1 = mpimg.imread(path_to_new_image)\n",
    "\n",
    "# Run object detection\n",
    "print(len([image1]))\n",
    "results1 = model.detect([image1], verbose=1)\n",
    "\n",
    "\n",
    "# Display results\n",
    "ax = get_ax(1)\n",
    "r1 = results1[0]\n",
    "visualize.display_instances(image1, r1['rois'], r1['masks'], r1['class_ids'],\n",
    "dataset.class_names, r1['scores'], ax=ax, title=\"Predictions1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
